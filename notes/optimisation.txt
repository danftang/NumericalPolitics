Problem: need an efficient optimisation algorithm for large numbers of
Q-learning agents.

Solution:
Start by defining a set of "Q-parameters" that defines a set of Q-functions, Q(state, action), of every agent. This in turn defines a set of agent policies which in turn defines a number of distributions over the 4-tuple <agent start-state, action, reward, end-state>) for each agent, one distribution for each attractor of the society. Given this distribution, and the Q-functions we can define the Q-loss for each agent, and so the collective Q-loss. Our aim is to find the Q-parameters that minimise this Q-loss [in-fact, a point at which each agent's Q-loss is at a minimum. This is necessarily a local minimum of the collective Q-loss but the converse is not necessarily true, actually, we're trying to satisfy a set of simultaneous (differential) equations. In fact, we have an objective function (i.e. Q-loss) for each agent which is potentially dependent on all Q-parameters, and wish to minimise each Q-loss w.r.t. the agent's individual Q-parameters. So, we have the gradient of collective Q-loss and gradients for each individual Q-loss. What if we move down the gradient of collective Q-loss, subject to individual Q-loss never increasing? If we make the individual Q-loss non-negative, and the collective Q-loss a linear sum of individual Q-losses, then the individual Q-loss gradients define a cone within which there exists a linear objective that reduces. As long as this cone is not empty, we choose a point within it.]

However, we don't know this distribution, and it depends on the Q-parameters of all the agents (a set of Q-parameters defines a distribution over training sets and vice-versa...there's a one-one mapping).



