\documentclass[a4paper]{article}
%\documentclass[a4paper]{report}
\usepackage{graphics}
\usepackage{breqn}
\setlength{\parindent}{0mm}
\setlength{\parskip}{1.8mm}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proof}{Proof}
\title{On model reduction and equivalence in ABMs}
%%\date{$28^{th}$ August, 2007}
\author{Daniel Tang}
%%\linespread{1.3}

\begin{document}
\maketitle
\section{Poisson models and Markov processes}

Let a Poisson model be a dynamic system on some discrete state space such that the probability that the process will transition from state $x$ to state $x'$ in time $dt$ is given by $\rho('x|x)dt$. If we let $\rho_{ij}$ be a transition matrix such that
\[
\rho_{ij} = 
\begin{cases}
\rho(i|j) & \text{if } i \ne j\\
\sum_{k \ne j} -\rho(k|j) & \text{if } i = j
\end{cases}
\]
and let $X$ be a vector representing a probability distribution over the state space, then we can define the Poisson model as a continuous time dynamic system
\[
\frac{dX}{dt} = \rho X
\]
so that
\[
X(t) = e^{\rho t}X(0) = e^{(\mu - I)rt}X(0) = \sum_{k=0}^\infty \frac{ (rt)^k e^{-rt}}{k!}\mu^kX(0)
\]
where $r = \max_j \sum_{k \ne j} \rho(k|j)$ is a scalar and $\mu = \frac{\rho}{r} + I$. It can be seen that each entry of $\mu$ is non-negative and the sum of entries in each column is 1, so $\mu^k X(0)$ is the state of a discrete time Markov process at time $k$.

Since $\mu rt$ commutes with $Irt$ we can separate and expand to give
\[
X(t) = e^{-rt}e^{\mu rt}X(0) = \sum_{k=0}^\infty \frac{ (rt)^k e^{-rt}}{k!}\mu^kX(0)
\]
which we recognize as a sum of powers of $\mu$ weighted with a Poisson distribution. So, each continuous-time Poisson model $\rho$ has an associated discrete-time Markov process $\mu = \frac{\rho}{r} + I$. The two are related by the fact that, for a given start state distribution, the state of the Poisson model at time $t$ is the weighted sum of states in a trajectory of the Markov process, where the weights are given by a Poisson distribution with rate $rt$.

This means, among other things, that as $t \to \infty$ the state distribution of a Poisson model tends to a uniform distribution over the attractor of its associated Markov model. So every Poisson model tends to a steady state distribution (i.e. a single point in distribution space).

\section{Social norms and social agents}

Let a \textit{social role} be a tuple $\left<S, \sigma_0, A, Q\right>$ where $S$ is a domain of social states of the actor of which $\sigma_0$ is the default state given at the beginning of an episode, $A$ is a domain of social actions that can be performed by the actor and $Q$ is a social-quality function in $S \times A \to \mathcal{R}$. 

Let a \textit{social norm} be defined as a tuple $\left<r_1, r_2, \tau \right>$ where $r_1$ and $r_2$ are social roles and $\tau$ is a state transition function in $S_1 \times A \times S_2 \to S_1 \times S_2$ saying that if two agents playing roles $r_1$ and $r_2$ are in states $s_1$ and $s_2$ respectively and agent 1 performs action $a$ on agent 2 then, after the action the agents have state $s'_1$ and $s'_2$ respectively. [can this be separated by defining the social quality of actions \textit{performed on other} given other's state? In this way, my behaviour towards you is constrained by your role, not mine. Can we define social state change solely by own actions? ]

Let a \textit{social connection} be a tuple $\left<N, s_1, s_2 \right>$ where $N$ is a social norm, $s_1$ is the social state of the first agent in $N$ and $s_2$ is the social state of the second.

Let a \textit{social network} be a directed graph, where each edge is associated with a social connection. So, each agent in a social network can be thought of as being in a number of social states (one for each edge) and each social state comes with a constraint on behaviour in the form of the social Q-functions.

Let a \textit{social agent} be a node in a social network. The agent's internal state is the set of social states on its edges. The agent's behaviour towards each agent it has a social connection with is constrained by the Q-function associted with that connection.

Agents in a social network need not, and often do not, physically exist. God, the government and Microsoft do not exist but many people model them as agents to which they have social connections. There raises no difficulty as long as the physical meaning of social actions involving the non-existant agent don't require its physical presence, and there's no reason it should need to. These actions can be instrumental in the coordination of behaviour between physically existant agents. I can be employed by, or sue Microsoft without ever expecting to meet face to face.

\subsection{Social network as closure}

If an agent models its world and assumes every other agent models the world in the same way, then evey agent in the model contains an instance of the model and we have an infinite regression, so we need some kind of closure.

If we assume that the state of the social network is public knowlede, through observation, gossip etc. (where X is public knowledge if everyone knows X and it's public knowlege that everyone knows X) then, under discounting, agents can model their world as consisting of agents that act according to the public knowledge. For example, agents can use the social norm as the off-tree approximator and run Monte-Carlo tree search to account for potential breaking of the social norm. In this way, agents are able to come to the same equilibrium.



\subsection{Social network as abstraction}

In many situations, we're only interested in the social actions that each agent performs, so we can model a set of agents as interacting via social actions. In reality, these social actions are grounded in physical actions and if we're interested in the details of this, then each social action must come with an extension which is a set of sequences of physical actions that qualify as this social action. The agent must then choose a physical action in consideration of the social interpretations of that action. In general, the physical substrate of the agent's social world adds additional constraints to the sequence of social actions it may perform.

Modelling at the physical level also allows agents to perform actions that have multiple social meanings, so a single physical action or sequence of actions can perform mamy social actions. It is also possible for a physical action to be unclassifiable as a social action yet state changing in the social norm (e.g. I sit down at a restaurant, the waiter brings me the menu, i set fire to it). In this case the norm is considered null and new norms need to be negotiated.

\section{How do agents learn social norms?}


[Language, telling stories. By accidentally breaking social norms and being told off. It seems implausible that we learn social norms only through personal experience and direct observation: I haven't learned not to murder people by murdering a few and finding that it wasn't really worth it, nor even by whatching people murder others and imagining that it isn't worth it. Rather, I learn it on the social level, as part of the culture in which I live.]

A learner of social norms that only has access to observed physical behaviours must somehow learn the abstraction function, the social quality function and the transition function. If thrown into a world where a social norm is already prevalent and stable, and we assume people are guided by social norms, then all this can theoretically be done by minimising the error in predicting other's behaviours.

Alternatively, can can abstract over the physical and assume agents make observations at the social level of abstraction. This glosses over the learning of the abstraction function, which we learn from gossip etc. and focusses on the social dynamics. If agents are able to make social observations of their surroundings and their interactions then an agent can construct a probabilistic policy for each social norm (or if the norm type is itself negotiated, and we have a universal state and action space, then this can be a single policy...although it's hard to see the advantage of this compared to stratification by norm).


\section{how do agents negotiate roles to form new social connections?}
Context. Verbally.

Again, we can abstract over the physical details of this and assume there is a public vocabulary of norms, and connections have an associated social norm type-id.

\section{How do social norms evolve?}

If agents choose to create social connections at a rate proportional to their expected reward then some social norms will be popular and others not. Ultimately, some may die out.

New norms can be created by more or less intelligent mutation of existing norms (perhaps even unintentional misunderstandings or chance events), or specialisation of general-purpose norms: [this assumes there's a heirarchy of abstraction among norms, which we haven't talked about yet...] for example, hunting large game and sharing the meat may emerge as a specialisation of you-help-me-and-i'll-help-you norm...?

\subsection{Social judgement in social norms}

In a purely Q-learning environment, punishment and praise after the fact is ineffective in one-shot games. However, if a social norm consists of a whole policy, and is agreed upon at the beginning of an interaction, then a credible threat of contingent punishment and/or praise can be instrumental in making the policy a Nash equilibrium.

Perhaps social judgement goes two ways: it rewards the actor for socially beneficial behaviour, but also signals to everyone which behaviours should be copied and which avoided. This makes sense if social judgement is outcome based and/or defined on an abstract level. That is, the ability to make social judements allows everyone to easily see whether a given behaviour is good or bad, but it is more difficult to generate instances of behaviours that are good.

\subsection{Justice}

It's clear that concepts such as justice are largely shared and preceed law (i.e. are more than a respect for the rule of law). It's also clear that humans don't define justice in terms of reward, but rather in terms of some kind of social value (Has justice been done if I give you two camels for your daughter?).

Moreover, a potential new social norm could conceivably conflict with other social norms (even though they have different state spaces), and so a new social behaviour cannot become established as a norm if it conflicts with already established norms. This implies that there is much more structure to social norms than we have allowed so far. What exactly is this structure, is it essential and how do we represent it?

Justice isn't a social norm as we've defined it so far in that it doesn't seem to have an action space. It is more a property of a story (sequence of interactions between a set of agents). Unless it is a formula for how to take an unjust story and ``serve justice'' to transoform it into a just story.

Or perhaps a person performs an unjust act on another, and justice is the social norm of punishment of the actor and/or redress to the injured. The player performing the punishment/redress can be abstract (e.g. government). These are the norms. The concept of justice help us think about these norms.

Or perhaps better to think of justice as a concept that is part of the conceptual apparatus we use to reason \textit{about} social norms, and perhaps to intellecutally explain our innate feelings.

\section{Generative abstraction}

To capture this, we propose \textit{generative abstraction} (which is something like the inverse of a formal semantics of natural language: while a semantics goes from a sentence to an extension, an abstraction goes from a set of observations to a representation. The abstraction is generative in the sense that the representation space grows exponentially with the size of the concept space). 

Suppose we have a complex dynamic system we wish to model. Let its state space be $T$. Suppose also that we have two abstract models with state spaces $A$ and $B$, that are different abstractions of $T$. We can create a new, compound model with state space $A \times B$ by simply allowing the state of the system to be a pair of states: its A-state and its B-state. A simple approximate dynamics for the compound model can be formed by just using the dynamics of the corresponding sub-models to update the $A$ and $B$ states independently. A good example of this is to model the dynamics of a 2-dimensional projectile in a field by splitting into perpendicular $x$ and $y$ dimensions. In this case, the dynamics is exactly ``factorizable'' (i.e. no error is introduced). 


If we interpret being in an abstract state as meaning that the target state is contained within the extension of the abstract state, and two abstract models are correct, then the simple dynamics of a compound model is also correct and the extension of the compound state is the intersection of the extensions of the constituent states. This is true for the componding of any number of models. Although some state combinations are impossible (the intersection of the extensions of the consitiuent states is empty), if we start in a possible state, we will never reach an impossible state if the constituent models are correct.

\section{Generative social norms}

If we allow an agent to simultaneously play multiple roles towards another agent, and it is public knowledge how the social Q functions of these roles combine, then the number of social Q functions that can be described increases exponentially with the size of the vocabulary of social norms. Note that social states do not have extensions, they describe a social judgement/requirement rather than a state of the world, so we need not worry about the abstractions of the states. However, since the action spaces of the norms aren't directly comparable, we need to decide how to combine action spaces. The easiest way is to make the action space the product of the constituent action spaces (perhaps with the addition of the null action in each dimension). This certainly covers all possible social actions, but can certain action combinations be contradictory? This is what we touched on earlier when we said that one social norm can be an instance of another. This can be modeled if we show that the state space and action space of one model is an abstraction of another, and that the social Q-functions are somehow non-contradictory under this abstraction relation [perhaps the Q-value of an abstract (state, action) pair is the mean of the Q-values of the members of the extension of the pair. i.e. this is the measure of the correctness of an abstraction, so we can measure all possible abstractions - also the dynamics needs to be an abstraction ]. Do such heirarchies help us make decisions by allowing us to decide on an abstract action first, or at least rule-out abstract actions?

\section{Social predicates}

The social state of a set of agents can be represented as a set of agent IDs (though, we'll see later that these may be inanimate objects or may not even exist) and a set of predicates on those IDs. A social predicate \texttt{pred(Alice,Bob)} is true iff Alice and Bob agree it is true. By agreeing to a predicate on oneself, one also agrees to take on the social role that goes with that predicate. Taking on a role is agreeing that one's acts will be socially judged by the social norms implied by the role.

Agent's actions can be split into two types: physical actions which change the physical state of subject and/or object, and social actions which change only the social predicates (abstracting over the physical brain state). To change the truth of \texttt{pred(Alice,Bob)}, Alice can suggest the change to Bob, if Bob agrees then the truth value changes by definition. The suggestion/agreement need not be verbal; if Alice lays out apples on a table with a price and stands behind it [we hardly notice here the social presumption of ``behind'' here rather than ``next to'' or even ``in-front of, with back facing observer''], she's suggesting to anyone who passes that \texttt{marketTrader(Alice,PasserBy)}. If Bob stands on the other side of the table (not the same side as Alice, even though this would be physically more convenient) and inspects the apples, he is agreeing to the suggestion and so it becomes true. In this way we build a whole imaginary social world, and the social world becomes more important to us than the physical.

By definition, any agent can unilaterally negate (but not assert) a predicate by simply withdrawing agreement (i.e. telling the other agent that it is no longer true), although that withdrawal may by socially unacceptable. However, if a social predicate depends on some phsical substrate for its truth, then an agent can forcibly negate the predicate by changing the physical substrate. If, in the above example, Mallory drives a car at high speed into the table of apples, then \texttt{marketTrader(Alice,Bob)} becomes false even though neither Alice nor Bob agreed. Mallory has performed a social as well as a physical action.

From these examples we see that there is a non-trivial relationship between physical actions and social actions. It's useful to distinguish between the objective physical world, which has its own dynamics without reference to social facts, and an agent's understanding of the physical world which, considering the importance of the social world, is likely to be instrumental in helping us understand the social world. It's also likely that we don't make a clear distinction between the physical and social worlds, the evidence being that we find it very hard to make any such distinction. The socio-physical world \textit{is} our world. This can be modelled if we allow ourselves to be in predicate relations to inanimate objects. The semantics isn't very different if we consider a suggestion to an inanimate object consists of our attempt to bring about a phsical state in relation to the object and the objective physical world ``decides'' whether to ``accept''. This is even reflected in our langauge, for example ``this nail is refusing to go in''. The difference is that certain predicates can only be filled by other agents (though we seem quite fluid even on this).

\subsection{What does it mean to be married to one's job?}


\subsection{Ownership}

Ownership involves a predicate between an agent and an inanimate object, \texttt{owns(Alice,Cake)}, but constrains the behaviour of third parties. Alice can consider herself the owner of a cake without further ado, but this in itself constrains no behaviour. If Alice meets Bob she can tell Bob that \texttt{owns(Alice,Cake)} and Bob can agree or dispute that. If Bob agrees, he is open to social judgement with respect to that ownership. Contrast this with the case when Alice suggests \texttt{pred(Alice,Bob)} to Bob. In a suggestion, Alice doesn't consider the suggestion to be true until Bob agrees, and Bob's agreement/rejection makes it true/false by definition. Whereas when Alice tells Bob something, she considers it to be true before the telling and irrespective of Bob's response.

Even if Bob agrees, he may still be physically able act on the cake and eat it. If he does, this will be judged as socially unacceptable, which will make it acceptable for Alice (or some representative of Alice) to punish Bob to the extent that justice has been served. Bob knows this and knows that this will likely happen so may decide not to eat the cake, even though he really likes cake.

Alice can give the cake to Bob by suggesting \texttt{-owns(Alice,Cake) \& owns(Bob,Cake)} and Bob agreeing (here \texttt{-} means rescind, rather than not).

Alice, because she owns the cake, can give Bob permission to eat it without relinquishing ownership. This is different from giving him the cake as she retains the right to rescind the permission. Permission requires the reification of actions as welll as the new action of giving permission and perhaps a higher order predicate of having permission.

\section{Social judgement and emotional reaction}

A social action, or a failure to act, can be judged by others, and Alice's judgement of Bob will affect her behaviour towards Bob. This is public knowledge so Bob will consider this when deciding how to act. But what exactly does this public knowledge consist of?

Judgement does not specify exact consequences. If Bob eats Alice's cake, it may not be clear to anybody exactly what the consequences will be, but everybody knows it is not socially acceptable and may affect Alice's juedgement of Bob. Alice can also tell others that Bob ate her cake, affecting their judgement of him too. Bob can deny it but only at the risk of compounding his transgression by being found out and branded a lier.

There is a severity to judgement, but not an exact quantification. It is clear that eating Alice's cake isn't as bad as hitting her with a baseball bat, but there isn't a well defined calculus: is using her toothbrush better or worse than eating her cake? What if it was her wedding cake? So, Bob should expect her to react until she feels appeased. Nobody knows when this will be but everyone has a rough idea of the average. It is for others to judge whether she is overreacting.

Alice's social judgement of Bob, then, is best described as a change in Alice's emotional attitude towards Bob which affects her behaviour towards Bob. Her emotional attitude toward Bob changes in response to Bobs actions toward her and her actions toward Bob. So, if agents measure the quality of their state in terms of other's emotional attitudes towards them, then at any instant they only need to choose the action that maximises some function of people's emotional attitudes and so the problem of living reduces to that of modelling other's emotional attitudes and seeing to ones bodily needs.

It is likely that we all share some innate emotional dynamic system that does not learn. Social norms, then, provide an interface between our socially constructed reality and our innate emotional system, allowing society to adapt through social change without having to evolve the emotional system through genetic change. The innate part can be modelled by giving each agent an emotional attitude towards each of its social ties. This attitude is changed by hard wired emotional reactions to certain changes to predicate states. Only a small vocabulary of ``innate'' predicates can be involved in the hard wired reactions. Social norms provide more complex social predicates and show how they are concretisations of the innate predicates. If agents are able to probe their own emotional reaction to an action, via it's effect on the innate predicates, then they are able to model others under the assumption that they have the same innate predicates and the same social norms. They are also able to model their own emotional state change in response to their social actions. All this gives the agent a basis on which to act.

Emotional reactions are themselves pleasant or unpleasent (rewarding/unrewarding) to the subject [is there also a direct emotional reaction to a perception of another's emotional reaction? Sympathy/empathy. Do we directly derive pleasure from others emotional reactions or do we react emotionally and the pleasure is from our own emotional reaction?]. Can we also derive pleasure simply from the belief in another's emotional state, even in the absence of any direct perception? If so, then pleasure may be the mechanism by which this all turns to action through maximisation of discounted pleasure. That is, the emotions of the group (or at least the individual's emotional state and his beliefs about other's emotions towards him) mediate the reward of the individual. However, if emotional attitude defines propensity to act then there is no need for pleasure to act, unless it is a mechanism to help the agent learn how to turn innate acts into concrete actions, or indeed to teach our emotional selves how to react emotionally to change [emotional state is the Q-vector]. Emotional state controls attention, in order to direct mental effort to the concretisation of innate acts.

Undirected emotion: it would seem there are emotions that aren't directed at anyone [or perhaps directed at a non existant object?]. If so, what purpose do they serve beyond mediating reward? They could direct behaviour that isn't interaction. E.g. if we allow hunger and thirst to be non-directed emotions, then I drink when I'm thirsty and eat when I'm hungry [what do i do when I'm angry and why?]. The implication is that there is an abstract action for each emotional dimension, and the emotional state is just the rate at which these actions are performed (and the triggering of an act changes the state) so we have a Poisson state-machine, and eveything we ever do is a (part of) a concretisation of a small vocabulary of absract, innate acts intended to alleviate some unpleasent emotional state. If this is so, then wouldn't a positive emotional state be the attractor which, once we reach it, we would cease to act further...i.e. what forces us away from positive emotional states into negative states? Hunger and thirst, boredom? The constant yearning for improvement? The arms race with others? If two adaptive agents are put in a zero-sum game, adaptation will continue until a Nash equilibrium is reached. However, if the Nash equilibrium does not satisfy either agent, then there will be an urge to act, but no way to act to satisfy that urge.

\section{Innate Roles}

Suppose that each agent has a set of innate social roles, whose state is expressed as abstract, innate predicates and whose actions are expressed as innate acts. The \textit{e-function} is a map from state predicates to emotional state, which is a vector of reals, each of which is associated with one of the acts in the action space, and which gives the rate of activation of each act.

Note that the emotional state takes the place of the Q-function. Emotional state is a function of innate predicated state, this abstraction helps us deal with, and learn from, complex social situations.
 
So, it should be possible to describe every history as a history of concretisations of innate social roles. What is this innate vocabulary of roles/norms? [fairy tales? What happens if we put a lot of agents together who all have the fairy tales hard wired?]

\section{Abstraction on a predicate space}

Let a social state be a tuple $(A, P_1, P_2)$, where $A$ is a set of agents, $P_1$ is a vector of relations in $A \to \{0,1\}$ (i.e. unary predicates over $A$) and $P_2$ is a vector of relations in $A \times A \to \{0,1\}$ (i.e. binary predicates over $A$). An social state can be made agent-centric by marking one of the agents as Self. So, given the sizes of $A$, $P_1$ and $P_2$ we can define a social state space.

Our aim is to define a tractable family of functions that assign emotional states to each agent given a social state.

A simple mechanism to abstract over a social state is to remove some agents from $A$ and remove all tuples that contain any removed agents from the extensions of the predicates. This makes most sense in agent-centric states, where we can, for example, remove agents that are unknown to Self.

We can also abstract over predicates by defining \textit{abstract predicates} whose extensions are those of sentences over the predicates of the state space, where the arguments of the predicates in the sentence refer to the arguments of the predicate (e.g. $IsAFruit(X) \equiv IsABanana(X) \vee IsAnApple(X) \vee ...$). This generates an abstract social state space with the same set of agents but a different (potentially smaller) set of predicates. This abstract space can itself be abstracted over in the same way, or by agent removal. This may require an extension to predicates with more than two arguments, but since we're defining their extension as a sentence rather than enumerating tuples, this poses no problem. This mechanism allows us to build a type heirarchy and typed predicates, among other things.

A function from an agent-centric social state to the reals can be described as an equation expressed as functions and sentences on the predicates. Where there is a sentence, its value is 1.0 if true and 0.0 if false.

\end{document}