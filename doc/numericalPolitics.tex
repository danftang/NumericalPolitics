\documentclass[a4paper]{report}
\usepackage{graphics, amsmath, amsthm, amssymb, rotating, url}
%%\usepackage[english,greek]{babel}
\setlength{\parindent}{0mm}
\setlength{\parskip}{1.8mm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
%%\newtheorem{proof}{Proof}
\title{Numerical Politics\\
	\large Numerical experiments in the politics of interacting, autonomous agents}
\author{Daniel Tang}
%%\linespread{1.4}
\begin{document}
%%\selectlanguage{english}
\maketitle
\tableofcontents

%%\begin{abstract}
%%\end{abstract}
\chapter{Introduction}

Numerical politics is a new approach to the study of politics in which we perform numerical experiments on simulated societies in order to gain an understanding of how organised, collective behaviour can emerge among interacting agents. This github repo (\url{https://github.com/danftang/NumericalPolitics}) presents a set of numerical experiments  that allows us to build an understanding of this emergent behaviour while also presenting the software and numerical techniques necessary to perform these experiments. Our approach will be to start with very simple agents in simple environments and gradually introduce more complex behaviours and complex environments. This will allow us to build up our theoretical understanding and introduce new numerical techniques in a logical order.

Eventually, this repo is intended to provide the software necessary to allow anyone to set up a numerical "laboratory" and start studying simulated societies. In these pages we'll also discuss how the practice of numerical politics can contribute to our understanding of how to effectively structure and govern real-world societies.

\section{The framework: thinking clearly about collective behaviour}

The subject matter of numerical politics is the collective behaviour of many interacting agents. Agents interact by passing messages between eachother. Each agent has a number of ``channels'' for receiving messages and any agent with the channel's ID can send a message down the channel. When an agent receives a message, it can respond by updating its internal state and/or sending yet more messages. More formally, the behaviour of an agent can be defined as a probability distribution, $P(a\|m,c,\psi)$, which is the probability that an agent in state $\psi$ performs action $a$ in response to receiving message $m$ in channel $c$. The action $a$ defines the agent's new internal state and/or a set of messages passed to other agent's channels at some time after receipt of the message. At first sight this may seem a bit abstracted from real-world application, but we choose it because it's a very flexible formalism that, on the one hand can easily be adapted to all applications, while on the other makes it easier to parallelise computations.

Our particular interest here will be to make predictive statements about individual and collective wellbeing of the agents. For this we assume there exists a function, $W(\psi)$, that is some measure of the wellbeing of an agent in state $\psi$ and define the collective wellbeing of a set of agents, $S$, as the sum of individual wellbeings $\Omega = \sum_{\psi \in S} W(\psi)$. It should be the subject of much debate exactly what the function $W(\psi)$ ought to be.

Notice that in this definition there is no mention of government. A goverment, if there is one, is encoded within the behaviours of the agents and is part of the model, as opposed to it being an exogenous actor imposing "policy interventions" on the agents. In this way, a government is best thought of as an emergent property of the agents' behaviours. We choose to make government endogenous because our interest here is \textit{not} to simulate specific policy interventions but to understand the fundamental principles of organised, collective behaviour.

\chapter{Sugar and spice world (version 1)}

We begin with an anarchic world with two commodities: sugar and spice. Suppose half the agents can farm only sugar and half can farm only spice, each at a rate of one unit per unit time. Suppose the agents randomly encouter eachother, whereupon each agent can either offer to trade or try to steal the other agent's crop. If both agents offer to trade then half the crop of each agent is swapped, however, if one agent offers to trade and the other tries to steal then the stealing agent gets half the other's crop and the other is left with only half its original crop. If both try to steal then they're both unsuccessful and no food is transferred. This is the classic prisoner's dilemma, an agent's wellbeing after an interaction is a function of the two agent's actions and is given in table \ref{prisonersdilemmareward}.

\begin{table}
\begin{center}
\begin{tabular}{lll}
Agent 1  & Agent 2 & Agent 1 wellbeing \\
\hline
trade & trade & 3 \\
trade & steal  & 0 \\
steal & trade & 4 \\
steal & steal & 1 \\
\hline
\end{tabular}
\end{center}
\caption{The wellbeing of an agent after an interaction}
\label{prisonersdilemmareward}
\end{table}

This world is simple enough that we can see immediately that the optimal collective wellbeing occurs when all agents trade. In this case, the average wellbeing of all agents is 3. However, under what circumstances will agents reach this optimum?

If we assume the agents are Q-learning then we can ask what kinds of society do the agents create for themselves using their learning. More mathematically we can look at the society as a dynamic system and ask about the distribution of wellbeing on the attractors. In the special case where the attractor is a point, we have a stable society where no amount of learning from further encounters will change any agent's policy.

\section{Zero memory agents}

If agents have no memory of previous encounters then each encounter is a simple prisoner's dilemma situation. The state of a Q-learning agent is just the Q-values of trade, $Q_t$ and steal, $Q_s$. Equilibrium is when
 
\[
Q_t = 3P(t) + r\max(Q_t, Q_s)
\]
\[
Q_s = 4P(t) + P(s) + r\max(Q_t, Q_s)
\]
but
\[
Q_s - Q_t = P(t) + P(s) = 1
\]
so $Q_s > Q_t$ irrespective of the other agent's behaviour so a zero memory Q-learning agent will always learn to steal, leading to a society where all agents try to steal and every agent is much worse off than in a trading society, with an average wellbeing of 1.

So, memoryless Q-learning agents get stuck in an equilibrium that is far from optimal both collectively and individually.

What needs to change in order to improve these agent's lives?

\section{One step memory agents}

If we give the agents the ability to remember the last encounter they had with another agent (if this isn't the first encounter) then the dynamics of the society gets much more interesting.

\subsection{Experiment 1: two agents}

We start with the simple case of just two agents. The agents begin with a high probability of exploring the policy space (by choosing a random action with uniform probability), and this probability reduces exponentially with time.

Experiment 1 in \url{https://github.com/danftang/NumericalPolitics} shows that under these circumstances, the agents quickly learn to trade by both adopting the policy described in table \ref{optimalpolicy1}. The first three entries in the table have analogues in human behaviour, but the last is a little unintuitive: if we both tried to steal from eachother last time, then this time I'll try to trade. This is key to the success of the policy as it means that whatever state the agents get into they quickly revert to mutual trading. As the exploration probability tends to zero, this society tends to the optimum of always trading, while remaining unexploitable (if I always try to steal from an agent with this policy, we'll flip between mutual stealing and me stealing from the agent, but my average wellbeing will be 2.5, less than if I take on the policy in table \ref{optimalpolicy1}).


\begin{table}
	\begin{center}
		\begin{tabular}{cccc}
My last move & Your last move & My next move & Human trait\\
\hline
trade & trade &  trade &	 mutual-benefit \\
trade & steal &  steal &	 revenge \\
steal & trade &  steal &	 exploitation \\
steal & steal &  trade &	 ? \\
\hline
\end{tabular}
\end{center}
\caption{The optimum behaviour of an agent with one-step memory}
\label{optimalpolicy1}
\end{table}

Note that the agents do not learn the tit-for-tat policy: if you tried to steal from me last time, I'll try to steal from you this time, but if you traded with me last time, I'll try to trade with you again. Mutual adoption of this strategy has three stable states: mutual trading, mutual stealing and alternating unilateral stealing $TS \rightarrow ST \rightarrow ...$. However, if the agents have a non-zero probability of exploring the policy space, then mutual tit-for-tat is not stable for Q-learners because, once in a run of mutual stealing, the Q-value of stealing again (and so getting into a long period of reward 1) is half the Q-value of trading (and so getting into a long period of reward (4+0)/2 = 2) so it makes sense to unilaterally trade with a tit-for-tat agent after mutual stealing. Ultimately, a pair of Q-learners will eventually learn to mutually adopt the behaviour in table \ref{optimalpolicy1}.

\subsection{Experiment 2: many agents}

Experiment 2 shows what happens as this society grows. We assume agents are able to recognize and remember the history of the first $n$ agents they meet, but after that everyone is a stranger.



\end{document}
