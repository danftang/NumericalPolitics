\documentclass[a4paper]{article}
%\documentclass[a4paper]{report}
\usepackage{graphics}
\usepackage{breqn}
%%\usepackage[english,greek]{babel}
\setlength{\parindent}{0mm}
\setlength{\parskip}{1.8mm}
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{proof}{Proof}
\title{Modelling interacting Poisson agents}
%%\date{$28^{th}$ August, 2007}
\author{Daniel Tang}
%%\linespread{1.3}

\begin{document}
%%\selectlanguage{english}
\maketitle
%%\tableofcontents

\section{Poisson models and Markov models}

Let $\mathcal{A}$ be the domain of agent states, and let an \textit{occupation vector}, $V$, be an $|\mathcal{A}|$-dimensional vector of integers so that The index of each element represents a state of an agent and the integer value at that index represents the number of agents in that state.

We define a Poisson agent as an agent whose state transition probability at any time can be expressed as a set of Poisson processes with rates $\rho_1..\rho_n$ so that in infinitesimal time $dt$ the probability of a transition due to the $i^{th}$ process is $\rho_i dt$. Furthermore, we assume that the processes can be expressed by the following two rate functions:
\begin{enumerate}
	\item $\rho_\psi(\Delta V)$ which is the rate at which an agent in state $\psi$ will perform an action that results in a perturbation in occupation vector of $\Delta V$ (i.e. $V' = V + \Delta V$) and
	
	\item $\rho_{\psi\phi}(\Delta V)$ which is the rate at which an agent in state $\psi$ will interact with an agent in state $\phi$ to produce a perturbation of $\Delta V$. If $\psi = \phi$ (i.e. it is an interaction with agents in the same state) then we allow interaction with self. If this is not intended, then the rate $\rho_\psi$ can be adjusted accordingly (with the consequence that we can have negative $\rho_\psi$ but not negative total rate).
\end{enumerate}


we also assume that all rates are non-negative and an agent can only be removed from the model by itself, so $\rho_\psi(\Delta V)$ can only be non-zero if all elements of $\Delta V + 1_\psi$ are non-negative, similarly $\rho_{\psi\phi}(\Delta V)$ can only be non-zero if all elements of $\Delta V + 1_\psi + 1_\phi$ are non-negative (where $1_\xi$ is the vector whose $\xi^{th}$ element is one, and all other elements zero).

Given the rates of an agent's processes, we can define the Poisson rates of transition, $\rho$, between occupation vectors in a model containing multiple agents, so that $\rho(V'|V)dt$ is the probability that a vector $V$ will transition to state $V'$ in time $dt$. So we have a Markov model over the domain of occupation vectors whose state changes are Poisson processes whise rates are given by 
\begin{equation}
\rho(W|V) = \sum_{\psi}  \rho_\psi(W-V) \gamma_1(V_\psi) + \sum_{\psi,\phi}  \rho_{\psi\phi}(W-V) \gamma_1(V_\psi)\gamma_2(V_\phi)
\label{rateEq}
\end{equation}
where $V_\psi$ is the value of $\psi^{th}$ element of $V$, $\gamma_1(n)$ models how an agent's rate is affected by other agents in the same state, $\gamma_2(m)$ models how an agent's rate is affected when in the presence of multiple other agents in the same state. For now we consider only $\gamma_i(n) = n$ (interaction is like a chemical reaction. We'll take this to be the default behaviour if not otherwise stated), $\gamma_1(n) = (n>0)$ (agents only care about the presence or absence of other agents, and multiplicity doesn't affect the rate) or $\gamma_i(n) = \min(n,n_{\text{max}})$. Note that $\gamma_i(0) = 0$ and $\gamma_i(1) = 1$ for any valid definition.

Given this, we can consider probability distributions over the domain of occupation vectors. The transition rates define a rate of change of a probability distribution,
\begin{equation}
\frac{d\Phi(V)}{dt} = \sum_{U} \rho(V|U)\Phi(U) - \sum_W \rho(W|V)\Phi(V)
\label{changeEq}
\end{equation}
In this way we can translate the agent based model into a dynamic system whose state space is the probability distributions over the domain of occupation vectors.

If we consider $\Phi$ to be a vector with an element for each occupation vector, and define $\rho$ to be a matrix\footnote{We implicitly suppose there is a ``flattening'' of the occupation vectors onto the integers to give row and column indices. However, we also implicitly suppose that the semantics of indices reproduces the semantics of the occupation vectors, so that in effect we can consider indices as having a dual life as integers and occupation vectors. In this spirit, we can think of vectors as also being fields over the non-negative gridpoints in an $|\mathcal{A}|$-dimensional space and matrices as also being functions of gridpoint pairs.} such that
\begin{equation}
\rho_{VU} = 
\begin{cases}
	\rho(V|U) & \text{if } U \ne V\\
	\sum_{W} -\rho(W|V) & \text{if } U = V
\end{cases}
\label{rhoMatrixEq}
\end{equation}
then equation \ref{changeEq} can be expressed as the matrix equation
\begin{equation}
	\frac{d\Phi}{dt} = \rho\Phi
	\label{expChangeEq}
\end{equation}

so that
\[
\Phi(t) = e^{\rho t}\Phi(0) = e^{(\mu - I)rt}\Phi(0) = e^{-rt}e^{\mu rt}\Phi(0)
\]
where $r = \max_V -\rho_{VV}$ is a scalar and $\mu = \frac{\rho}{r} + I$ and the last step is valid since $\mu$ commutes with $I$.

Expanding the exponential in $\mu$ gives
\[
\Phi(t)  = \sum_{k=0}^\infty \frac{ (rt)^k e^{-rt}}{k!}\mu^k\Phi(0)
\]
which we can recognize as a sum of powers of $\mu$ weighted with a Poisson distribution. However, since each entry of $\mu$ is non-negative and the sum of entries in each column is 1, $\mu^k \Phi(0)$ can be thought of as the state of a discrete time Markov process after $k$ steps. So, each continuous-time Poisson model $\rho$ has an associated discrete-time Markov process $\mu = \frac{\rho}{r} + I$ such that the state of the Poisson model at time $t$ is the weighted sum of states in a trajectory of the Markov process, where the weights are given by a Poisson distribution with rate $rt$.

This means, among other things, that if the Markov model has an attractor then as $t \to \infty$ the state distribution of the corresponding Poisson model tends to an equilibrium distribution, and that distribution is the uniform probability over the attractor of its Markov model. So every Poisson model tends to a steady state distribution (i.e. a single point in distribution space).

The occupation vectors can be thought of as points in an $|\mathcal{A}|$-dimensional space, which makes $\Phi$ a probability field over an $|\mathcal{A}|$-dimensional grid on the non-negtive integer coordinate points in this space. With this in mind, we can re-express the multiplication by $\mu$ in terms of a convolution.

\subsection{Markov step as convolution}

We can expand $\rho\Phi$ using equations \ref{rateEq} and \ref{rhoMatrixEq} 
\begin{dmath}
	(\rho\Phi)_V = 
	\sum_{V'}\left(
	\sum_{\psi}  \rho_\psi(V-V') \gamma_1(V'_\psi)
	+ \sum_{\psi,\phi}  \rho_{\psi\phi}(V'-V) \gamma_1(V'_{\psi})\gamma_2(V'_{\phi})
	\right)\Phi_{V'} 
	-
	\left(
	\sum_{\Delta V,\psi}  \rho_\psi(\Delta V) \gamma_1(V_\psi) 
	+ \sum_{\Delta V,\psi,\phi}  \rho_{\psi\phi}(\Delta V) \gamma_1(V_\psi) \gamma_2(V_\phi)
	\right)
	\Phi_V
\end{dmath}
so, if we let the ``transition kernels'' be fields over the occupation vectors
\[
\tau^{\psi\phi}_{\Delta V} = 
\begin{cases}
	\rho_{\psi\phi}(\Delta V) & \text{if } \Delta V \ne \vec{0}\\
	-\sum_{\Delta V'} \rho_{\psi\phi}(\Delta V') & \text{if } \Delta V = \vec{0}
\end{cases}
\]
and
\[
\tau^\psi_{\Delta V} = 
\begin{cases}
	\rho_\psi(\Delta V) & \text{if } \Delta V \ne \vec{0}\\
	-\sum_{\Delta V'} \rho_\psi(\Delta V') & \text{if } \Delta V = \vec{0}
\end{cases}
\]
then we can write
\begin{dmath}
	(\rho\Phi)_V = 
	\sum_{V'}\left(
	\sum_{\psi}  \tau^\psi_{(V-V')} \gamma_1(V'_\psi)
	+ \sum_{\psi,\phi}  \tau^{\psi\phi}_{(V'-V)} \gamma_1(V'_{\psi})\gamma_2(V'_{\phi})
	\right)\Phi_{V'} 
\end{dmath}

Now let $n_{i\psi}$ denote an $|\mathcal{A}|$-dimensional field such that $n_{i\psi}(V) = \gamma_i(V_\psi)$ so that the sums over $V'$ become $|\mathcal{A}|$-dimensional convolutions
\begin{equation}
	\rho\Phi = 
	\sum_{\psi}\tau^\psi \ast (n_{1\psi}\Phi)
	+ \sum_{\psi,\phi}  \tau^{\psi\phi} \ast (n_{1\psi}n_{2\phi}\Phi)
\end{equation}
where multiplication of fields is to be interpreted as being pointwise.

Since
\[
\mu\Phi = \left(\frac{\rho}{r} + I\right)\Phi = \frac{1}{r} \rho\Phi + \Phi
\]
then a timestep of the Markov model is given by
\[
\Phi^{t+1} = \mu\Phi^t = \Phi^t + 
\frac{1}{r}\sum_{\psi}\tau^\psi \ast (n_{1\psi}\Phi^t)
+ \frac{1}{r}\sum_{\psi,\phi}  \tau^{\psi\phi} \ast (n_{1\psi}n_{2\phi}\Phi^t)
\]

From here we assume $n_{1\psi} = n_{2\psi}$ and just write $n_\psi$ for convenience.

\subsection{Distributions as Bernstein polynomials}
If we represent the fields as stretched Bernstein polynomials then, in the space of Bernstein coefficients, multiplication by $n_{i\xi}$ corresponds to shifting by one along the $\xi$ axis.

Convolution needs to be defined carefully as we're only interested in the integer points within the range of interest, so our aim is to find the lowest order polynomial that fits the integer points.

\[
B_{m,n} \ast B_{m'n'} =   {m \choose n}{m' \choose n'} \int_{0}^x y^m(1-y)^{n-m} (x-y)^{m'}(1-(x-y))^{n'-m'} dy
\]

\subsection{Chebyshev polynomials at the Chebyshev points}
How about we represent as Chebyshev polynomials, but interpret the integer values at the Chebyshev points, so that we can easily go from space to frequency domain.

\subsection{Fermionic agents}

If we restrict ourselves to the unit hypercube, then a pre-requisite of a process being applicable is that any created agents are not already present. This will come out as higher order interactions, where some of the interactions involve absence of agents rather than presence.

In this case, the aggregators, $n_i$, can be expressed as $B_{1,1}=x$ or $B_{0,1}=(1-x)$ and transitions are expressed again in delta space. That is, we can always fit a polynomial that is linear in any one variable. In the Bernstein basis, fitting only the points at 0 and 1 we have 
\[
B_{0,1} \ast B_{0,1} = B_{0,1}
\]
\[B_{0,1} \ast B_{1,1} = B_{1,1} \ast B_{0,1} = B_{1,1}\]
\[
B_{1,1} \ast B_{1,1} = 0
\]
similarly for multiplication
\[
B_{0,1}B_{0,1} = B_{0,1}
\]
\[
B_{0,1}B_{1,1} = B_{1,1}B_{0,1} = 0
\]
\[
B_{1,1}B_{1,1} = B_{1,1f}
\]

\subsection{Fermionic agents with bosonic interaction}

If we both fermions and bosons in a simulation, but interaction can only happen between bosons and fermions that are in the same interaction state, then the state of a model becomes the joint state of the bosonic and fermionic fields. Interactions then will always be between a fermionic axis and a bosonic axis. If we also require that a boson is absorbed on interaction (and emission must be a separate process) then bosonic axes only ever multiply by x and shift downwards.

\subsection{Distributions as operators on the vacuum}
Since most occupation vectors of interest will have most elements zero, and most distributions will only have probability mass at points where most elements are zero, then we want to represent these states compactly. This is not the case with a simple polynomial basis.

One way to deal with this is to express the terms as operators on (convolutions with) the vacuum.

\subsection{Distribution as fluid}
Since the convolution kernels all sum to zero, probability mass will act like a compressible fluid with conserved mass in occupation number space. If we add the constraint that any process may only add at most one agent in any state then, along with the constraint that an agent may only delete itself, we can calculate a divergence field and localised stability constraints. The divergence field can be separated into the sum of devergence contributions from each kernel.

The kernels can be expressed in terms of finite difference operators, thus giving us an approximate link to a set of differential equations on a continuous field.

[Can we characterise as an advection/diffusion equation by splitting the kernels into a random walk component plus an advection component?]

\subsection{Deterministic vs probabilistic processes}

So far we've been assuming processes are stochastic in the sense that the result is probabilistic and this is reflected in the transition kernels $\tau$. This is equivalent to separate deterministic process with different rates so we can express convolutions in the form:
\[
\tau \ast \Phi = \sum_k \rho_k(s_k(\Phi) - \Phi)
\]
where each $s_k$ shifts the field in occupation number space.

Furthermore, since agents can only remove themselves from the model we have
\[
\tau^\psi \ast (n_\psi\Phi) = \rho(s^+(\Phi) - \Phi)s^-_\psi n_\psi \Phi 
\]
and
\[
\tau^{\psi\phi} \ast (n_\psi n_\phi \Phi) = \rho(s^+(\Phi) - \Phi)s^-_\psi n_\psi s^-_\phi n_\phi \Phi 
\]


So we can use any basis that allows shifts (we'll write $s^+_i$ and $s^-_i$ for a shift in the +ve and -ve direction in the $i^{th}$ dimension respectively), multiplication by $n_i$, addition and multiplication by a real. Exponential polynomials [need to be careful not to import from the -ve occupation numbers on shift]. Deselby distributions. Exponentially decaying spectral. Polynomial times Heaviside step function. Shifted Chebyshev/Bernstein polynomial. Scaled Chebyshev/Bernstein.

Discrete spectral approximation over a finite domain works well as shifting is multiplication by $e^{-i\omega \Delta t}$ and anything shifted out reappears on the other side of the domain, so if we only shift out zeroes we automatically get zeroes shifting in the other end.

Bernstein polynomials can be shifted by noting that any mass shifted in will show up in just one basis, so we can then subtract a best approximation to a delta function.

Note that a field that can be factored into the product of lower dimensional fields remains factorizable under each of the operations apart from addition (so can be expressed as the sum of factored fields).

\subsection{The operator basis}

Let a compound operator be an ordered list of $s$ and $n$ operators. The compound operators are not linerly independent since we have the commutation relations
\[
[n_i,s^+_i] = n_i s^+_i - s^+_i n_i = s^+_i  
\]
\[
[s^-_i, n_i] = s^-_i n_i - n_i s^-_i = s^-_i  
\]
\[
[n_i, n_j] = 0
\]
\[
[s^*_i, n_{j \ne i}] = 0
\]
and
\[
s^-s^+_i
\]
where $s^*$ stands for either of $s^+$ or $s^-$.

So, we can choose a subset of linearly independent compound operators that forms a basis that spans the set of all compound operators. Let the \textit{canonical compound operators} be the ones whose $n$ operators all preceed the $s$ operators (in order of operation on the field). Since the ordering of the $n$ and $s$ operators among themselves makes no difference, we can choose a canonical ordering, and identify a canonical compound operator as a pair of integer $|\mathcal{A}|$-dimensional vectors, $N$ and $S$, where the $i^{th}$ element of $N$ gives the number of $n_i$ operators

So, all sequences of field operation are equivalent to  a weighted linear sum of the canonical compound operators. So, we call the canonical compound operators an \textit{operator basis}.

Note that the vacuum field (the one with 1 at the origin and 0 elsewhere) is the unique field that is annihilated by all $n_i$ operators.

\section{Public states and Interaction states}

We can distinguish between different classes of interaction rate function $\rho_{\psi\phi}$ in order to understand how the properties of this function affect the ABMs dynamics, and also to make computationally efficient algorithms.

It may be possible to split the state of an agent into public and private states, $\psi = \left<\psi_u, \psi_l\right>$ so that the interaction rate depends only on the public state of the other agent $\rho_{\psi\left<\phi_u,\phi_l\right>} = \rho'_{\psi\phi_u}$, thereby reducing the dimensionality. Occupation vectors can then be represented as matrices where the row gives the public state and the column gives the private state.

A speical case of this is when only agents with the same public state can interact. In this case, we call the public state an ``interaction state''.

\section{Simulating}

Given an occupation vector and a set of transitions with associated rates, the probability of a transition along edge $i$ in time $dt$ is $\rho_i dt$ and the probability that there is no transition in $dt$ is $1-\sum_k \rho_k dt$, so the probability that $i$ is the next transition is
\[
P_n(i) = \sum_{t=0}^\infty (1-\sum_k \rho_k dt)^t \rho_i dt = \frac{\rho_i}{\sum_k \rho_k}
\]
so we just need to choose a transition with probability proportional to the rate.

If we express transitions as perturbations to the occupation vector, then on transition most transitions remain unchanged, so we can implement this by storing a distribution over transitions from which we draw a transition and use the chosen transition to perturb the dsitribution.

\section{Spatial agents}

Suppose agents are in a spatial environment (e.g. a 2D grid) and that only agents on the same grid-square can interact. Within a gridsquare, all pairwise interactions are possible, but the rates are dependent on the non-spatial internal states of the agents. An event can consist of a single method call, or a whole turns-based encounter.

If an event is just a method call, then even binary events change only the state of a single agent. which may simplify parallelisation. In this case every agent has a local time set to the time of its last state-change. The time of its next state change is dependent only on the rates of its own state-changing processes which are defined by the states of the other agents in the same grid-square at the agent's current local time. So, a grid-square needs to ``remember'' the states of all agents at each of the local times of the contained agents. An agent cannot step forward until the rates of all its interactions are known.

If an event is a whole binary encounter, then we have improved computational efficiency (more useful computation per event, no need to remember who we're talking with inbetween events, no need for high rate responses). However, if encounters are non-deterministic, given the full state of both agents, then we need to encode probabilistic consequences of an event (this could be encoded by simulation of the encounter). If agents are identifiable, an agent can have high rates of interaction with known others.

In either case, a binary interaction has a primary and a secondary agent (i.e. an interaction is not symmetrical, one of the agents initiates the interaction, so an interaction of A and B is not necessarily the same as an interaction of B and A). It is also up to the initiator to set the rate of the interaction i.e. the rate of a binary interaction of A and B can depend on the complete state of A but only the public state of B.

\subsection{Parallelism in spatial ABMs}

Suppose we assign local times to grid-squares so that all agents within a gridsqure are at the same gridsquare local time. Each gridsquare is computed with at most one thread. We specify that it takes time $\Delta t$ for an agent to move from one gridsquare to another, during which time no other events may occur to the agent. So a gridsquare can calculate up to its ``frontier'' which is $\Delta t$ ahead of the earliest local time of its neighbouring squares. In this way we get a ``light cone'' constraint.

Each gridsquare has a local time, $t$, a frontier time, $t_f$, a set of neighbours that this square is currently blocking and an ``incoming agent'' list. If $t=t_f$ we say the square is blocked. After an event is processed for a gridsquare, a ``next event'' is drawn for the interval $[t,t_f]$. If the set of Poisson processes contained in the gridsquare have rates $\rho_1...\rho_N$ and $R = \sum_i \rho_i$ then the probability that the next event occurs in the interval $[\tau, \tau+d\tau]$ from the current time is given by
\[
P_i(\tau)d\tau = Re^{-R\tau}d\tau
\]

So, draw a time offset $\tau$ from this exponential, if $t + \tau > t_f$ then the gridsquare becomes blocked on the earliest neighbour (randomly chosen if there are multiple earliest neighbours). Otherwise, $t \to t + \tau$, a process is chosen with probability $\frac{\rho_i}{R}$ and the event is added to a set of events to be processed.

If the gridsquare's incoming-agent list is not empty and the next incoming agent is at time  $t < t' < t_f$ then we treat the interval $[t, t']$, add the agent if no intervening event occurs, otherwise process the event, and repeat.

When a gridsquare's local-time is updated, any neighbours that are blocked on this one are notified of the new local time. When a blocked gridsquare receives an unblocking notification, it updates its frontier and draws an event as above, possibly blocking again and possibly sending further unblocking notifications.

A simulation begins with all gridsquares at $t=0$ and all frontiers at $t_f=\Delta t$. Each gridsquare is drawn from to create the initial set of events to be processed, so that all squares are either blocked or have an event to be processed.

If an agent moves from gridsquare $i$ to gridsquare $j$, then the agent is removed from $i$ and added to the destination gridsquare's incoming-agent list with timestamp $t_i + \Delta t$.

\section{Computing over distributions}

\subsection{Computing with non-local basis}



\section{Computation for studying social norms}

[What do we want to calculate? How should we use computation to do sociology?

Existance proof that an observed social phenomenon does not require a given individual property. Proved by simulation of individuals that lack the given property yet robustly (i.e. not by luck/chance) demonstrate the given observed social behaviour.

Calculation of posterior over individual behaviour given a prior and a set of social observations, in the form of timeseries or of properties of the attractor. Can be used to imply or engineer individual behaviour.

Data assimilation, given individual behaviour and timeseries observation (or perhaps only prior over behaviour).

Social engineering over social norms: Given a space of social norms, an individual behaviour (prior?) and an objective function, find a set of social norms that maximises the expectation of the objective function on the attractor [do societies ever reach the attractor? Can we expect all point on an attractor to have the same social norms (i.e. is it a point in social norm space)?]. 

Stability analysis: Given a set of social norms, how stable is this set to deviant behaviour, subcultures, endogenous evolution, exogenous forcing?

Engineering Revolution: Given a set of social norms, find the smallest (a small) perturbation, or set of small perturbations that leads to a new set of norms.

]


%%\appendix

\end{document}
